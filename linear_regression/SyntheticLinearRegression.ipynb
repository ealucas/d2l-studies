{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Synthetic data* can help us to evaluate the properties of our learning algorithms and to confirm that our implementations work as expected.\n",
    "\n",
    "First, we define the *SyntethicRegressionData* class, its input tensor $X$ and output $y = Xw + b + noise$.\n",
    "Using this class, we generate our data with arbitrary $w$ and $b$ values, we store our data as a tensor using *TensorDataset*, and load our dataset into the *DataLoader* -> an iterable that abstracts the complexity of handling minibatches, reshuffling the data at every epoch, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class SyntheticRegressionData(nn.Module):\n",
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000, batch_size=32):\n",
    "        super().__init__()\n",
    "        n = num_train + num_val\n",
    "        self.X = torch.randn(n, len(w))\n",
    "        noise = torch.randn(n, 1) * noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1,1))) + b + noise\n",
    "        self.batch_size = batch_size\n",
    "        #w.reshape((-1,1)) changes w to a column vector with shape [n,1]\n",
    "        \n",
    "\n",
    "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n",
    "dataset = TensorDataset(data.X, data.y)\n",
    "dataloader = DataLoader(dataset, batch_size=data.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a *LinearRegression* class. We fill our weights with values samples from a normal distribution an our bias with zeros. *nn.LazyLinear* is fully connected layer with 1 output feature. It automatically infers __in_features__ during the first pass. When we call forward, it applies a linear transformation. We set our loss function as *nn.MSELoss()* and set *torch.optim.SGD* as our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/stuff/miniconda3/envs/d2ls/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, lr):\n",
    "        super().__init__()\n",
    "        self.net = nn.LazyLinear(1)\n",
    "        self.net.weight.data.normal_(0, 0.01)\n",
    "        self.net.bias.data.fill_(0)\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        fn = nn.MSELoss()\n",
    "        return fn(y_hat, y)\n",
    "    \n",
    "    def configure_optimizers(self, lr):\n",
    "        return torch.optim.SGD(self.parameters(), lr)\n",
    "\n",
    "model = LinearRegression(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "configure_optimizers() missing 1 required positional argument: 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 10\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, dataloader)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train the model using the provided DataLoader.\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model  \u001b[38;5;66;03m# Store model inside Trainer\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_optimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Initialize optimizer\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Initialize epoch counter\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs):  \u001b[38;5;66;03m# Train for num_epochs\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: configure_optimizers() missing 1 required positional argument: 'lr'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, num_epochs=10, lr=0.01):\n",
    "        self.num_epochs = num_epochs  # Store the number of epochs\n",
    "        self.lr = 0.01\n",
    "\n",
    "    def fit(self, model, dataloader):\n",
    "        \"\"\"Train the model using the provided DataLoader.\"\"\"\n",
    "        self.model = model  # Store model inside Trainer\n",
    "        self.optimizer = self.model.configure_optimizers(lr=self.lr)  # Initialize optimizer\n",
    "        self.epoch = 0  # Initialize epoch counter\n",
    "\n",
    "        for self.epoch in range(self.num_epochs):  # Train for num_epochs\n",
    "            self.fit_epoch(dataloader)\n",
    "\n",
    "    def fit_epoch(self, dataloader):\n",
    "        \"\"\"Perform one training epoch.\"\"\"\n",
    "        self.model.train()  # Set model to training mode\n",
    "        \n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(dataloader):\n",
    "            self.optimizer.zero_grad()  # Reset gradients\n",
    "            y_pred = self.model(X_batch)  # Forward pass\n",
    "            loss = self.model.loss(y_pred, y_batch)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            self.optimizer.step()  # Update model parameters\n",
    "            \n",
    "            print(f\"Epoch {self.epoch+1}/{self.num_epochs}, Batch {batch_idx+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "trainer = Trainer(num_epochs=10)\n",
    "trainer.fit(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2ls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
